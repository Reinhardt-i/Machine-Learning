{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Logistic Regression Intuition**"
      ],
      "metadata": {
        "id": "Qb9SWr0Tq0MS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import math\n",
        "from matplotlib.patches import FancyArrowPatch\n",
        "from ipywidgets import Output"
      ],
      "metadata": {
        "id": "3oNXGvtjhdP1"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "XC_lTr4Ug0y7"
      },
      "outputs": [],
      "source": [
        "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  #(m,n)\n",
        "y_train = np.array([0, 0, 0, 1, 1, 1])  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_label=\"y=1\"\n",
        "neg_label=\"y=0\"\n",
        "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
        "\n",
        "pos = y_train == 1\n",
        "neg = y_train == 0\n",
        "print(pos.shape)\n",
        "\n",
        "# Plot examples\n",
        "ax.scatter(X_train[pos, 0], X_train[pos, 1], marker='x', s=80, c = 'red', label=pos_label)\n",
        "ax.scatter(X_train[neg, 0], X_train[neg, 1], marker='o', s=80, label=neg_label, facecolors='none', edgecolors='#0096ff', lw=3)\n",
        "ax.legend(loc='best' )\n",
        "\n",
        "ax.figure.canvas.toolbar_visible = False\n",
        "ax.figure.canvas.header_visible = False\n",
        "ax.figure.canvas.footer_visible = False"
      ],
      "metadata": {
        "id": "N8B5xPjAg_pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    z = np.clip( z, -500, 500 )           # protect against overflow\n",
        "    g = 1.0/(1.0+np.exp(-z))\n",
        "    return g"
      ],
      "metadata": {
        "id": "K2HMCPDSkUnz"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost_logistic(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes cost\n",
        "\n",
        "    Args:\n",
        "      X (ndarray (m,n)): Data, m examples with n features\n",
        "      y (ndarray (m,)) : target values\n",
        "      w (ndarray (n,)) : model parameters  \n",
        "      b (scalar)       : model parameter\n",
        "      \n",
        "    Returns:\n",
        "      cost (scalar): cost\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[0]\n",
        "    cost = 0.0\n",
        "    for i in range(m):\n",
        "        z_i = np.dot(X[i],w) + b\n",
        "        f_wb_i = sigmoid(z_i)\n",
        "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
        "             \n",
        "    cost = cost / m\n",
        "    return cost"
      ],
      "metadata": {
        "id": "ySyzbmRIkHYD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_tmp = np.array([1,1])\n",
        "b_tmp = -3\n",
        "print(compute_cost_logistic(X_train, y_train, w_tmp, b_tmp))"
      ],
      "metadata": {
        "id": "ZrmI5JhMjBpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose values between 0 and 6\n",
        "x0 = np.arange(0,6)\n",
        "\n",
        "# Plot the two decision boundaries\n",
        "x1 = 3 - x0\n",
        "x1_other = 4 - x0\n",
        "\n",
        "fig,ax = plt.subplots(1, 1, figsize=(4,4))\n",
        "# Plot the decision boundary\n",
        "ax.plot(x0,x1, c='#0096ff', label=\"$b$=-3\")\n",
        "ax.plot(x0,x1_other, c='#FF40FF', label=\"$b$=-4\")\n",
        "ax.axis([0, 4, 0, 4])\n",
        "\n",
        "# Plot the original data\n",
        "\n",
        "ax.scatter(X_train[pos, 0], X_train[pos, 1], marker='x', s=80, c = 'red', label=pos_label)\n",
        "ax.scatter(X_train[neg, 0], X_train[neg, 1], marker='o', s=80, label=neg_label, facecolors='none', edgecolors='#0096ff', lw=3)\n",
        "ax.legend(loc='best' )\n",
        "\n",
        "ax.figure.canvas.toolbar_visible = False\n",
        "ax.figure.canvas.header_visible = False\n",
        "ax.figure.canvas.footer_visible = False\n",
        "\n",
        "ax.axis([0, 4, 0, 4])\n",
        "ax.set_ylabel('$x_1$', fontsize=12)\n",
        "ax.set_xlabel('$x_0$', fontsize=12)\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.title(\"Decision Boundary\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CPNIYUeWkoMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_array1 = np.array([1,1])\n",
        "b_1 = -3\n",
        "w_array2 = np.array([1,1])\n",
        "b_2 = -4\n",
        "\n",
        "print(\"Cost for b = -3 : \", compute_cost_logistic(X_train, y_train, w_array1, b_1))\n",
        "print(\"Cost for b = -4 : \", compute_cost_logistic(X_train, y_train, w_array2, b_2))"
      ],
      "metadata": {
        "id": "KC3ztZ8tkwiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IiF9Lx1WqtTT"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradient Descent**"
      ],
      "metadata": {
        "id": "lrSooZM3rA6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient_logistic(X, y, w, b): \n",
        "    \"\"\"\n",
        "    Computes the gradient for linear regression \n",
        " \n",
        "    Args:\n",
        "      X (ndarray (m,n): Data, m examples with n features\n",
        "      y (ndarray (m,)): target values\n",
        "      w (ndarray (n,)): model parameters  \n",
        "      b (scalar)      : model parameter\n",
        "    Returns\n",
        "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
        "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n",
        "    \"\"\"\n",
        "    m,n = X.shape\n",
        "    # print(m,n)\n",
        "    dj_dw = np.zeros((n,))                           #(n,)\n",
        "    dj_db = 0.\n",
        "\n",
        "    for i in range(m):\n",
        "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
        "        err_i  = f_wb_i  - y[i]                       #scalar\n",
        "        for j in range(n):\n",
        "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
        "        dj_db = dj_db + err_i\n",
        "    dj_dw = dj_dw/m                                   #(n,)\n",
        "    dj_db = dj_db/m                                   #scalar\n",
        "        \n",
        "    return dj_db, dj_dw"
      ],
      "metadata": {
        "id": "viKhciCfrEhY"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n",
        "    \"\"\"\n",
        "    Performs batch gradient descent\n",
        "    \n",
        "    Args:\n",
        "      X (ndarray (m,n)   : Data, m examples with n features\n",
        "      y (ndarray (m,))   : target values\n",
        "      w_in (ndarray (n,)): Initial values of model parameters  \n",
        "      b_in (scalar)      : Initial values of model parameter\n",
        "      alpha (float)      : Learning rate\n",
        "      num_iters (scalar) : number of iterations to run gradient descent\n",
        "      \n",
        "    Returns:\n",
        "      w (ndarray (n,))   : Updated values of parameters\n",
        "      b (scalar)         : Updated value of parameter \n",
        "    \"\"\"\n",
        "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
        "    J_history = []\n",
        "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
        "    b = b_in\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "        # Calculate the gradient and update the parameters\n",
        "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n",
        "\n",
        "        # Update Parameters using w, b, alpha and gradient\n",
        "        w = w - alpha * dj_dw               \n",
        "        b = b - alpha * dj_db               \n",
        "      \n",
        "        # Save cost J at each iteration\n",
        "        if i<100000:      # prevent resource exhaustion \n",
        "            J_history.append( compute_cost_logistic(X, y, w, b) )\n",
        "\n",
        "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
        "        if i% math.ceil(num_iters / 10) == 0:\n",
        "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
        "        \n",
        "    return w, b, J_history         #return final w,b and J history for graphing"
      ],
      "metadata": {
        "id": "XNFbKCYJCnHK"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_tmp  = np.zeros_like(X_train[0])\n",
        "b_tmp  = 0.\n",
        "alph = 0.1\n",
        "iters = 10000\n",
        "\n",
        "w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters) \n",
        "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
      ],
      "metadata": {
        "id": "RNmxGIgGCyky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,1,figsize=(5,4))\n",
        "\n",
        "# Plot the original data\n",
        "ax.set_ylabel(r'$x_1$')\n",
        "ax.set_xlabel(r'$x_0$')   \n",
        "ax.axis([0, 4, 0, 3.5])\n",
        "\n",
        "# Plot examples\n",
        "ax.scatter(X_train[pos, 0], X_train[pos, 1], marker='x', s=80, c = 'red', label=pos_label)\n",
        "ax.scatter(X_train[neg, 0], X_train[neg, 1], marker='o', s=80, label=neg_label, facecolors='none', edgecolors='#0096ff', lw=3)\n",
        "ax.legend(loc='best' )\n",
        "\n",
        "ax.figure.canvas.toolbar_visible = False\n",
        "ax.figure.canvas.header_visible = False\n",
        "ax.figure.canvas.footer_visible = False\n",
        "\n",
        "# Plot the decision boundary\n",
        "x0 = -b_out/w_out[0]\n",
        "x1 = -b_out/w_out[1]\n",
        "ax.plot([0,x0],[x1,0], c='#0096ff', lw=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ATEsTvP3C4ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q0hOTsynDQDu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}